# AI-Powered Visual Search Engine

## ğŸ“‹ Project Overview
A scalable visual search system that finds visually similar images using CLIP embeddings and FAISS vector database. This system is optimized for fashion images using the DeepFashion dataset and works efficiently on both CPU and GPU environments (RTX 3050+ recommended for optimal performance).

## ğŸ—ï¸ Architecture
- **Frontend**: Streamlit web interface with intuitive search functionality
- **Backend**: FastAPI with async endpoints and comprehensive error handling
- **ML Pipeline**: CLIP embeddings with PyTorch and optimized batch processing
- **Database**: FAISS vector database with persistent local storage
- **Deployment**: Local development environment

## âœ¨ Features
- **Real-time Visual Search**: Upload an image and find similar fashion items instantly
- **REST API**: Complete API with interactive documentation
- **Scalable Architecture**: Handles 100K+ images efficiently
- **GPU Optimized**: Automatic GPU detection and optimization
- **Comprehensive Logging**: Error handling and performance monitoring
- **Easy Setup**: Simple installation and configuration process

## ğŸ“Š Performance & Specifications
- **Hardware**: Optimized for RTX 3050+ (4GB VRAM), works on CPU
- **Memory**: 8GB+ RAM recommended, 16GB for large datasets
- **Batch Processing**: Dynamic batch sizing (8-32 images based on hardware)
- **Search Latency**: <100ms for similarity search
- **Dataset Support**: 100K+ images with room for scaling
- **Storage**: Efficient FAISS indexing with 512-dimensional embeddings

## ğŸ”§ Technical Implementation
- **Vector Database**: FAISS (Facebook AI Similarity Search) for high-performance similarity search
- **Storage**: Local persistent storage with automatic save/load functionality
- **Similarity Metric**: Cosine similarity using normalized embeddings
- **Index Type**: Flat index for exact search results
- **No External Dependencies**: Self-contained database that doesn't require separate server processes

## ğŸš€ Quick Start

### Prerequisites
- **Python**: 3.9+ (3.11 recommended for best performance)
- **Memory**: 8GB+ RAM (16GB recommended for large datasets)
- **GPU**: NVIDIA GPU optional but recommended (RTX 3050+ for optimal performance)
- **Storage**: 2GB+ free space (more for larger datasets)
- **OS**: Windows 10+, macOS 10.14+, or modern Linux distribution

### 1. Clone and Setup
```bash
# Clone the repository
git clone <your-repo>
cd visual-search-engine

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Verify setup
python scripts/verify_setup.py
```

### 2. Dataset Setup
Download the **DeepFashion Category and Attribute Prediction Benchmark**:
- Download from: [Google Drive](https://drive.google.com/drive/folders/0B7EVK8r0v71pWGplNFhjc01NbzQ?resourcekey=0-BU3lAk-Nc7HscJu-CyC1yA&usp=sharing)
- Extract to `data/deepfashion/`

**Expected folder structure:**
```
data/deepfashion/
â”œâ”€â”€ Anno_coarse/              # Main annotation files
â”‚   â”œâ”€â”€ list_category_cloth.txt      # 50 clothing categories
â”‚   â”œâ”€â”€ list_category_img.txt        # Image-category mappings
â”‚   â”œâ”€â”€ list_attr_cloth.txt          # 1000+ clothing attributes
â”‚   â”œâ”€â”€ list_attr_img.txt            # Image-attribute mappings
â”‚   â”œâ”€â”€ list_bbox.txt                # Bounding box annotations
â”‚   â””â”€â”€ list_landmarks.txt           # Fashion landmark annotations (8 landmarks)
â”œâ”€â”€ Anno_fine/                # Fine-grained train/val/test splits
â”‚   â”œâ”€â”€ train.txt, val.txt, test.txt  # Data partition files
â”‚   â”œâ”€â”€ train_*.txt, val_*.txt, test_*.txt  # Split-specific annotations
â”‚   â””â”€â”€ list_*.txt              # Category and attribute definitions
â”œâ”€â”€ Eval/                     # Evaluation protocols
â”‚   â””â”€â”€ list_eval_partition.txt      # Overall train/val/test splits
â”œâ”€â”€ Img/                      # ~289K fashion images (~5,620 categories)
â”‚   â””â”€â”€ [clothing_category_folders]/  # Images organized by item names
â””â”€â”€ README.txt                # Official dataset documentation
```

### 3. Run the Pipeline

#### Quick Start (Recommended)
```bash
# Navigate to project directory
cd visual-search-engine

# Option 1: Quick test with 1000 images
python scripts/run_pipeline.py --data-dir ./data/deepfashion --max-images 1000 --complete

# Option 2: Full pipeline with all images (slow, 2+ hours)
python scripts/run_pipeline.py --data-dir ./data/deepfashion --complete
```

#### Step-by-Step Execution
If you prefer to run individual steps:
```bash
# Step 1: Process dataset metadata
python scripts/run_pipeline.py --data-dir ./data/deepfashion --step data

# Step 2: Generate embeddings
python scripts/run_pipeline.py --data-dir ./data/deepfashion --step embeddings

# Step 3: Setup vector database
python scripts/run_pipeline.py --data-dir ./data/deepfashion --step database

# Step 4: Validate system
python scripts/run_pipeline.py --data-dir ./data/deepfashion --step validate
```

#### Launch Services
After completing the pipeline, start both services:

**Option 1: Quick Start (Recommended)**
```bash
# Terminal 1: Start API server
python -m uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8001

# Terminal 2: Start web interface
streamlit run frontend/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
```

**Option 2: Background Services**
```bash
# Start API in background
start /b python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8001

# Start Streamlit in background
start /b streamlit run frontend/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
```

ğŸŒ **Access Your Application:**
- **Web Interface**: http://localhost:8501
- **API Documentation**: http://localhost:8001/docs
- **API Health Check**: http://localhost:8001/health

### 4. Access Services & Usage

#### Web Interface (Recommended for Users)
- **URL**: http://localhost:8501
- **Features**: Upload images, browse results, adjust similarity threshold
- **Best For**: End users, demonstrations, testing

#### API Interface (For Developers)
- **Base URL**: http://localhost:8001
- **Documentation**: http://localhost:8001/docs (Interactive Swagger UI)
- **Health Check**: http://localhost:8001/health
- **Search Endpoint**: POST /search with image upload
- **Best For**: Integration, automation, custom applications

#### Local Database
- **Location**: `data/processed/`
- **Type**: FAISS vector index with metadata
- **Size**: ~50MB for 10K images, scales linearly

### 5. Common Parameters

| Parameter | Description | Example |
|-----------|-------------|---------|
| `--max-images` | Limit number of images (for testing) | `1000` |
| `--batch-size` | Images per batch (adjust for your GPU) | `16` |
| `--step` | Run specific step only | `data`, `embeddings`, `database`, `validate` |
| `--complete` | Run all pipeline steps | (flag only) |

### 6. Troubleshooting

**Import Error Fix:**
```bash
# Ensure you're in the project root directory
cd visual-search-engine

# If still failing, set Python path explicitly
set PYTHONPATH=%CD% && python scripts/run_pipeline.py --data-dir ./data/deepfashion
```

**Memory Issues:**
```bash
# Reduce batch size for lower VRAM
python scripts/run_pipeline.py --data-dir ./data/deepfashion --batch-size 8 --max-images 500 --complete
```

**Database Issues:**
```bash
# Clear database and start fresh (if needed)
rmdir /s data\processed\chroma_db\
```

**Quick Setup Verification:**
```bash
# Run verification script to check setup
python scripts/verify_setup.py
```

## ğŸ—ƒï¸ Database Information

The system uses **FAISS (Facebook AI Similarity Search)** for vector storage:
- **No external server required**: Runs entirely locally
- **High performance**: Optimized for similarity search  
- **Persistent storage**: Automatically saves and loads index data
- **Simple setup**: No configuration needed

### Database Files:
```
data/processed/chroma_db/
â”œâ”€â”€ faiss_index.bin        # FAISS index with embeddings
â”œâ”€â”€ metadata.json          # Image metadata
â””â”€â”€ id_mapping.json        # ID mappings
```

### Migration from ChromaDB to FAISS:
If you previously used ChromaDB, the system has been updated to use FAISS for better reliability and easier setup. No manual migration is needed - simply run the pipeline steps again and the system will automatically create the new FAISS database.

## ğŸ› ï¸ Development

### Manual Service Startup
```bash
# Start API server (Terminal 1)
python -m uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8001

# Start web interface (Terminal 2)  
streamlit run frontend/streamlit_app.py --server.port 8501
```

### Project Structure
```
visual-search-engine/
â”œâ”€â”€ ğŸ“ data/                     # Dataset and processed files
â”‚   â”œâ”€â”€ deepfashion/             # DeepFashion dataset (download required)
â”‚   â””â”€â”€ processed/               # Generated embeddings and database
â”‚       â”œâ”€â”€ embeddings.npy       # CLIP embeddings
â”‚       â”œâ”€â”€ metadata.csv         # Image metadata
â”‚       â”œâ”€â”€ image_mappings.json  # Image path mappings
â”‚       â””â”€â”€ chroma_db/           # FAISS vector database
â”œâ”€â”€ ğŸ“ src/                      # Core application code
â”‚   â”œâ”€â”€ api/                     # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ main.py              # API application entry point
â”‚   â”‚   â”œâ”€â”€ endpoints.py         # API route definitions
â”‚   â”‚   â””â”€â”€ schemas.py           # Pydantic models
â”‚   â”œâ”€â”€ embeddings/              # CLIP model and embedding generation
â”‚   â”‚   â”œâ”€â”€ clip_model.py        # CLIP model wrapper
â”‚   â”‚   â””â”€â”€ generator.py         # Embedding generation pipeline
â”‚   â”œâ”€â”€ database/                # Vector database operations
â”‚   â”‚   â”œâ”€â”€ vector_db.py         # FAISS database wrapper
â”‚   â”‚   â””â”€â”€ indexer.py           # Indexing operations
â”‚   â”œâ”€â”€ data/                    # Data processing utilities
â”‚   â”‚   â””â”€â”€ preprocessor.py      # Dataset preprocessing
â”‚   â””â”€â”€ utils/                   # Shared utilities
â”‚       â”œâ”€â”€ image_utils.py       # Image processing helpers
â”‚       â””â”€â”€ logger.py            # Logging configuration
â”œâ”€â”€ ğŸ“ frontend/                 # Streamlit web interface
â”‚   â””â”€â”€ streamlit_app.py         # Main web application
â”œâ”€â”€ ğŸ“ scripts/                  # Automation and setup scripts
â”‚   â”œâ”€â”€ run_pipeline.py          # Main pipeline orchestrator
â”‚   â””â”€â”€ verify_setup.py          # Setup verification
â”œâ”€â”€ ğŸ“ tests/                    # Test suite
â”‚   â”œâ”€â”€ test_api.py              # API tests
â”‚   â”œâ”€â”€ test_embeddings.py       # Embedding tests
â”‚   â”œâ”€â”€ test_database.py         # Database tests
â”‚   â””â”€â”€ test_api_integration.py  # Integration tests
â”œâ”€â”€ ğŸ“ docs/                     # Documentation
â”‚   â””â”€â”€ DATASET_ANALYSIS.md      # Dataset structure analysis
â”œâ”€â”€ ğŸ“ extras/                   # Optional features
â”‚   â””â”€â”€ monitoring/              # Prometheus/Grafana configs
â”œâ”€â”€ ğŸ“ docker/                   # Docker configurations (optional)
â”‚   â”œâ”€â”€ api.Dockerfile           # API container
â”‚   â””â”€â”€ frontend.Dockerfile      # Frontend container
â”œâ”€â”€ ğŸ“ logs/                     # Application logs
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ docker_compose.yml        # Docker Compose configuration
â””â”€â”€ ğŸ“„ README.md                 # This file
```

### Running Tests
```bash
pytest tests/
```

## ğŸ“š References

- [DeepFashion Dataset](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html)
- [DeepFashion Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf)
- [CLIP Paper](https://arxiv.org/abs/2103.00020)
- [FAISS Documentation](https://faiss.ai/)

## ğŸ“ Citation

If you use this system or the DeepFashion dataset, please cite:

```bibtex
@inproceedings{liu2016deepfashion,
  author = {Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang},
  title = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
  booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2016} 
}
```

## ğŸ“ License

MIT License - see LICENSE file for details.
